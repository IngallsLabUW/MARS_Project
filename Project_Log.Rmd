---
title: "MARS Project Log"
author: "RLionheart"
date: "10/22/2020"
output:
  html_document: default
  pdf_document: default
---

### MARS Project Log

- Retention times on the MS2 sheet and the Ingalls standards sheet have some variability. Should I incorporate the collision energy into our ingalls standards, or be using that as a parameter on the experimental data (if it exists)? 
- Total Similarity Score: include individual similarities in output? Example: Uracil as a 1:1 similarity on everything except MS2, which has a relatively poor similarity. That brings it down to .80 rather than a 1. These details can be an output of a potentially relational table? Is it worth filtering for collision energy (which MoNA has)?

This is an ongoing log: a place for build notes and questions.
*This is not to be confused with the Design Doc on the Shared Google Drive; that is for overall design direction and team suggestions. It is also not a README, which will be written closer to project completion.*

What is being worked on right now

Streamlining the process for import and standards-related ID.
1. Standard import file with feature, mz, rt, column, z, and ms2
*Confidence Level 1*

- Use [FuzzyJoin](http://varianceexplained.org/fuzzyjoin/) to match m/z, rt, column, and z for positive IDs and return a dataframe of matches. 

*Similarity Score*

- For the overall spectral similarity score, a combination of several equations from KRH and Tsugawa et. al form a total similarity equation for scoring.

*Confidence Level 2*

- Match unknown experimental values to MoNA and Metlin dataframes (which will be updated either every quarter, or as often as necessary). 

MoNA: Bare bones run complete
Metlin: in progress

- Only for mass features that have MS2 (otherwise not eligible for "level 2" confidence match.

*Confidence Level 3*
KEGG: in progress


#### Ongoing issues
Issues that will need to be fixed

- Incorporate existing KRH code to easily update the standards MS2 data.
  + This step will require cleaning, including but not limited to no intensities below 1 (throw out all those tiny fragment spectra), scale intensity so the biggest value is 100, and then order by descending intensity.
- Decide on order of matching parameters: column, z, mz, rt, ms2? 
- Set up flexible equation for Total.Similarity score.
- The dataframes to be annotated are from a variety of sources (xcms, MSDIAL), and are presented in a variety of formats and curation levels. Input will need to be standardized.
- Need to replace old compound names with new ones!
- For now, using given tolerance values is fine, but eventually we should be calculating it from the fuzzyjoin tolerance *clarify this later with KRH*.
- For now, we can compare to our own spectra and use 0.02 as tolerance. Revisit later to make it into ppm space.
- Incorporate the existing Metlin scraping structure into a database that will grow with each successive scrape.
- Run through the KEGG matching functions. 
- Tidy and rename variables in nearly all steps.

#### Other notes
- "w/o MS2:" on identified compounds is from MSDIAL output and currently isn't matching that we should rely on.
- Remember, you can get the score w/out MS2! The eventual full function should be able to do both.
- Will can make an MS2 library for stds
- Naming schemes are all different for unique mass features
- Potential change for rt times to nano energy
- Spectra are relative abundances.
- For DDA high resolution, the mass spec sees a high signal. When it see that, it takes above and below that signal and isolates it in the collision chamber. In the ms2 df, the first column is the exact mass with an intensity of 100 (maximum), then second mass and intensity, etc. 
- "From literature" ranking stats: Eventually this should be some kind of format to manually enter interesting stuff. 
- Moving to rank 3: currently nothing exists for HMDB Poke around the website. Might be connected to MoNA. Will is going to send some stuff for PubMed, but it's a huge download that may not be worth it. Try everything else first.
- With regards to MS2: Individual run #s at end are different collision energies
Lots of the unknowns aren't even real peaks. Some of them might be? Hypothetically there shouldn't be anything but standards, but there is noise and junk in there.
Alignment results first? Look at those, peak ID. Probably can get rid of unknowns in the alignment results.
Peak ID should be used as the key to find the correct individual results. That should be the "key", and then the MSMS data is in the individual samples. New standards MS2: It's actually 12 total injections for pos and 12 for neg since we have two different standard mixes. 
- Output should be relational, or a complete csv depending on who's looking/filtering. Wrapper in package should account for that. ie, how many matches come up and where are they from?
